{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q_Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as ps\n",
    "import numpy as np\n",
    "import random as rndm\n",
    "import copy as cp\n",
    "import time as time\n",
    "from colorama import Back, Style\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINNISH_REWARD=10\n",
    "OBSTACLE_REWARD=-10\n",
    "PRIZE_REWARD=10\n",
    "EMPTY_REWARD=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class environment:\n",
    "    def __init__(self,file_location):\n",
    "        self.init_map(file_location)\n",
    "        self.loc=(0,0)\n",
    "        self.got_prize=False\n",
    "        self.actions=[(1,0),(0,1),(-1,0),(0,-1)]\n",
    "        \n",
    "    def init_map(self,file_location):\n",
    "        map_file=open(file_location,\"r\")\n",
    "        map_str=map_file.read()\n",
    "        map_rows=map_str.split(\"\\n\")\n",
    "        self.map=[]\n",
    "        for row in map_rows:\n",
    "            self.map.append(row.split())\n",
    "\n",
    "    def get_state_size(self):\n",
    "        return len(self.map)*len(self.map[0])*2\n",
    "    \n",
    "    def get_reward(self,loc):\n",
    "        curr_block=self.map[loc[0]][loc[1]]\n",
    "        if(curr_block=='.'):\n",
    "            return EMPTY_REWARD\n",
    "        if(curr_block =='F'):\n",
    "            return FINNISH_REWARD if self.got_prize else EMPTY_REWARD\n",
    "        if(curr_block =='P'):\n",
    "            return PRIZE_REWARD\n",
    "        if(curr_block =='O'):\n",
    "            return OBSTACLE_REWARD\n",
    "\n",
    "    def is_done(self,loc):  \n",
    "        curr_block=self.map[loc[0]][loc[1]]\n",
    "        return (self.got_prize and curr_block =='F') or curr_block =='O'\n",
    "\n",
    "    def get_next_state(self,action):\n",
    "        next_loc=tuple(map(sum, zip(self.loc, action)))\n",
    "        next_prize=self.got_prize\n",
    "        reward=self.get_reward(next_loc)\n",
    "        if(self.map[next_loc[0]][next_loc[1]]=='P'):\n",
    "            next_prize=True\n",
    "            self.map[next_loc[0]][next_loc[1]]='.'\n",
    "\n",
    "        self.loc = next_loc\n",
    "        self.got_prize = next_prize\n",
    "        return (next_loc[0], next_loc[1], int(next_prize)), reward, self.is_done(next_loc)\n",
    "\n",
    "    def get_current_state(self):\n",
    "        return (self.loc[0], self.loc[1], int(self.got_prize))\n",
    "\n",
    "    def get_valid_actions(self):\n",
    "        valid=[True,True,True,True]\n",
    "        if(self.loc[0]==0):\n",
    "            valid[2]=False\n",
    "        if(self.loc[1]==0):\n",
    "            valid[3]=False\n",
    "        if(self.loc[0]==len(self.map)-1):\n",
    "            valid[0]=False\n",
    "        if(self.loc[1]==len(self.map[0])-1):\n",
    "            valid[1]=False\n",
    "        \n",
    "\n",
    "        valid_actions=[]\n",
    "        for i in range(4):\n",
    "            if valid[i]:\n",
    "                valid_actions.append(self.actions[i])\n",
    "        \n",
    "        return valid_actions\n",
    "\n",
    "\n",
    "    def print(self):\n",
    "        for i in range(len(self.map)):\n",
    "            for j in range(len(self.map)):\n",
    "                if (i,j)==self.loc :\n",
    "                    print(Back.GREEN+self.map[i][j],end='')\n",
    "                    print(Style.RESET_ALL,end='')\n",
    "                else:\n",
    "                    print(self.map[i][j],end='')\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES_COUNT=20\n",
    "MAX_STEP=100\n",
    "\n",
    "LEARNING_RATE = 0.1\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "EPSILON = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_index(state,env):\n",
    "    return state[0]+len(env.map)*state[1]+len(env.map)*len(env.map[0])*state[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_Learn(initial_env):\n",
    "    Q_table=np.zeros((initial_env.get_state_size(), (len(initial_env.actions))))\n",
    "    log=[]\n",
    "    for _ in range(EPISODES_COUNT):\n",
    "        env=cp.deepcopy(initial_env)\n",
    "        episode_reward=0\n",
    "        for _ in range(MAX_STEP):\n",
    "            clear_output(wait=True)\n",
    "            env.print()\n",
    "            time.sleep(0.1)\n",
    "            state=env.get_current_state()\n",
    "            state_index=get_state_index(state,env)\n",
    "            if rndm.uniform(0,1)<EPSILON:\n",
    "                action_index=int(rndm.uniform(0,3.99))\n",
    "            else:\n",
    "                if(np.min(Q_table[state_index,:])==np.max(Q_table[state_index,:])):\n",
    "                    action_index=int(rndm.uniform(0,3.99))\n",
    "                else:\n",
    "                    action_index=np.argmax(Q_table[state_index,:])\n",
    "\n",
    "            valid_actions=env.get_valid_actions()\n",
    "            new_state,reward,done= env.get_next_state(valid_actions[action_index%len(valid_actions)])\n",
    "            episode_reward+=reward\n",
    "            print(episode_reward)\n",
    "            Q_table[state_index][action_index]=(1-LEARNING_RATE)*Q_table[state_index][action_index]+LEARNING_RATE*(reward+DISCOUNT_FACTOR*np.max(Q_table[get_state_index(new_state,env),:]))\n",
    "\n",
    "            if(done):\n",
    "                clear_output(wait=True)\n",
    "                env.print()\n",
    "                time.sleep(0.1)\n",
    "                print(reward)\n",
    "                break\n",
    "\n",
    "        log.append(episode_reward)\n",
    "    return log\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....\n",
      ".O.O\n",
      "....\n",
      "...\u001b[42mF\u001b[0m\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "env=environment(\"map.txt\")\n",
    "Q_Learn(env);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
